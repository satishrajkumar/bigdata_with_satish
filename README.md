<!-- Hero Banner Image: Top of the README -->
<img src="assets/hero_banner.png" alt="Hero Banner" width="800">
---

## ğŸ‘¨â€ğŸ’» About Me
<a id="about"></a>
Iâ€™m a Results-driven Data Engineer with over 5 years of experience designing and optimizing ETL/ELT pipelines, handling large-scale structured and semi-structured data, and implementing cloud-native solutions.  

**Specialties include:**
- Building robust data pipelines using Python, PySpark, SQL
- Working across modern cloud stacks: AWS Glue, Lambda, S3, EC2, GCP, BigQuery  
- Developing scalable solutions on Databricks and Snowflake 
- Orchestrating workflows with Apache Airflow
- Driving data quality, performance tuning, and cost-optimized design  

> Proven track record: Up to **30% reduction in ETL runtime** and highly reliable pipelines (**99.9% uptime**).

---

## ğŸ“ Education
- B.E., BMS College of Engineering (2014â€“2018)

---
## ğŸ† Certifications
- **SnowPro Core Certification** â€“ Snowflake  

---

# Work Experience
<a id="experience"></a>

## **Publicis Sapient, Hyderabad, India**  
**Sr. Data Engineer (2024-Present)**

### Projects
### 1.ğŸ§ Podcast Listener Analytics â€“ Audience Platform Data Lake
**Tech Stack:** PySpark, Databricks, AWS S3, AWS Lambda  


The Audience platform was built as a single source of truth data lake for all business units. It centralizes audience and podcast listener data and enables enterprise-wide reporting, advanced analytics, and data science model training.

- Built scalable ingestion and transformation pipelines  
- Designed multi-layer architecture: Landing â†’ Silver â†’ Analytical
- Implemented listener mapping algorithms across multiple sources  
- Orchestrated ETL workflows using Databricks Workflows  
- Optimized query performance with PySpark and SQL tuning


### 2.ğŸ”„ Redshift to Salesforce Migration
**Tech Stack:** AWS Glue, AWS S3, Python Bulk API v2, PySpark  

- Migrated 1TB member records across Salesforce objects  
- Used AWS Glue workflows for scalable ETL  
- Implemented bulk insert/upsert logic via Python Bulk API v2  
- Achieved <1% error rate in production (Certificate of Excellence)  

---

## **Ernst & Young, Bangalore, India**  
**Data Engineer (2022-2024)**

### Projects
### 1.ğŸ§® IFRS17 Compliance Platform â€“ Financial Services
**Tech Stack:** AWS Glue, PySpark, Athena, RDS, Airflow  

- Built pipelines for IFRS17 regulatory compliance  
- Used Athena + RDS for efficient querying and storage  
- Automated workflows with Apache Airflow
- Established robust data validation frameworks 


### 2.Customer & Order Analytics Platform on GCP
**Tech Stack:** GCP, Cloud Composer (Apache Airflow), Python, GCS, BigQuery, SQL, Looker Studio

### Project Description
Built a cloud-native analytics platform to serve as a centralized source of truth for customer and order data.  

### Key Achievements
- Designed end-to-end ETL pipelines for daily customer & order data  
- Orchestrated workflows via Cloud Composer 
- Cleaned and transformed data in Python, standardized columns, handled missing values  
- Loaded datasets into BigQuery, optimized for analytics  
- Created analytical SQL transformations for reporting  
- Ensured fault-tolerant execution with error handling & retries 
- Exposed datasets for Looker Studio dashboards

### 3.ğŸ“Š Metadata Dashboard Automation â€“ BI4BI
**Tech Stack:** Power BI, SQL Server, Python, REST APIs  

- Automated metadata ingestion using Python + REST APIs 
- Delivered Power BI dashboards for key metrics  
- Enabled actionable insights through Pandas and SQL transformations  

---

## **Subtle Solutions, Pune, India**  
**Role:** Data Engineer (2019-2022)

### Projects
### 1.E-Commerce Analytics & Data Platform on AWS
**Tech Stack:** AWS Glue, PySpark, Python, S3, RDS, Lambda, Athena, EC2, SQL  

**Project Description**  
Developed a scalable AWS data platform to support analytics, reporting, and operations.  

**Key Achievements**
- Built scalable ETL pipelines using AWS Glue and PySpark  
- Ingested data from multiple commerce systems into RDS & analytics stores
- Implemented data quality checks & validation rules*
- Optimized PySpark jobs and SQL queries  
- Automated tasks using AWS Lambda, validated via Amazon Athena  
- Monitored production pipelines and fixed issues with minimal downtime  
- Delivered analytics-ready datasets for downstream reporting  

---

## ğŸŒ GitHub Pages Portfolio Website
<a id="portfolio"></a>

ğŸ”— [https://mallinathnpatil1996.github.io](https://mallinathnpatil1996.github.io) 

---

## ğŸ“« Contact Me
<a id="contact"></a>
- ğŸ“§ **Email**: [mallinathnpatil1996@gmail.com](mailto:mallinathnpatil1996@gmail.com)  
- ğŸ”— **LinkedIn**: [linkedin.com/in/yourprofile](https://linkedin.com/in/yourprofile)  

---

## ğŸ… Achievements

- Certificate of Excellence â€“ Publicis Sapient  
- Successfully migrated 17M+ records with <1% error  
- Reduced ETL runtime by 30% across multiple pipelines  

---

## ğŸ¨ Hobbies & Interests

- Data visualization & dashboards  
- Open-source contributions  
- Reading technical blogs & research papers  
- Playing chess  

---
## ğŸ› ï¸ Skills & Tools
<a id="skills"></a>

| Category              | Tools & Technologies                               |
|-----------------------|-----------------------------------------------------|
| Programming           | Python, SQL, PySpark, Pandas                        |
| Cloud                 | AWS (Glue, S3, Lambda, EC2), Snowflake              |
| Big Data              | Databricks, Hadoop, Spark                           |
| Workflow Orchestration| Apache Airflow, AWS Glue Workflows                 |
| Visualization         | Power BI                                            |
| Databases             | Redshift, RDS, SQL Server                          |
| Version Control       | Git, GitHub, JIRA                                   |


## ğŸ“œ License

This repository is open-source and available for educational and portfolio purposes. Please attribute if referencing or reusing code.

---

## ğŸ™ Acknowledgments

Thanks to mentors, peers, and teams who shaped my professional journey and these projects.

